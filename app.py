from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "API is running!"}

@app.get("/ping")
async def ping():
    return {"message": "I'm alive!"}

# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù† Hugging Face Ù…Ø¹ ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
MODEL_NAME = "bishoy1/swimming_coach"

quantization_config = BitsAndBytesConfig(load_in_8bit=True)  

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=quantization_config,
    device_map="auto"
)

# ğŸ”¹ ØªØ¹Ø±ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„Ø©
class ChatRequest(BaseModel):
    message: str

# ğŸ”¹ API Endpoint Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
@app.post("/chat")
async def chat(request: ChatRequest):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(request.message, return_tensors="pt").to(device)
    
    outputs = model.generate(
        **inputs, 
        max_length=150, 
        temperature=0.7, 
        top_p=0.9, 
        repetition_penalty=1.2
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}
